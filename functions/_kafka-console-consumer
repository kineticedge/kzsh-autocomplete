#compdef kafka-console-consumer

function _extract_arg() {
  local arg=$1
  local index=1
  while [[ $index -le ${#words[@]} ]]; do
    if [[ ${words[$index]} == $1 ]]; then
      echo "${words[$((index + 1))]}"
      return
    fi
    ((index++))
  done
}

function _groups() {
  local BROKER=$(_extract_arg "--bootstrap-server")
  local CONFIG_FILE=$(_extract_arg "--consumer.config")
  local -a COMMAND=(--bootstrap-server "$BROKER")
  if [[ -n "$CONFIG_FILE" ]]; then
    COMMAND+=(--command-config "$CONFIG_FILE")
  fi
  local -a groups
  if [[ -x "$(command -v kaflist)" ]]; then
    groups=($(kaflist groups "${COMMAND[@]}"))
  elif [[ -x "$(command -v kafka-consumer-groups)" ]]; then
    groups=($(kafka-consumer-groups "${COMMAND[@]}" --list 2>/dev/null))
  elif [[ -x "$(command -v kafka-consumer-groups.sh)" ]]; then
    groups=($(kafka-consumer-groups.sh "${COMMAND[@]}" --list 2>/dev/null))
  else
    groups=()
  fi
  compadd ${groups[@]}
}


function _topics() {
  local BROKER=$(_extract_arg "--bootstrap-server")
  local CONFIG_FILE=$(_extract_arg "--consumer.config")
  local -a COMMAND=(--bootstrap-server "$BROKER")
  if [[ -n "$CONFIG_FILE" ]]; then
    COMMAND+=(--command-config "$CONFIG_FILE")
  fi
  local -a topics
  if [[ -x "$(command -v kaflist)" ]]; then
    topics=($(kaflist topics "${COMMAND[@]}"))
  elif [[ -x "$(command -v kafka-topics)" ]]; then
    topics=($(kafka-topics "${COMMAND[@]}" --list 2>/dev/null))
  elif [[ -x "$(command -v kafka-topics.sh)" ]]; then
    topics=($(kafka-topics.sh "${COMMAND[@]}" --list 2>/dev/null))
  else
    topics=()
  fi
  compadd ${topics[@]}
}


function _deserializer() {
  local configs=(
    "org.apache.kafka.common.serialization.BooleanDeserializer"
    "org.apache.kafka.common.serialization.ByteBufferDeserializer"
    "org.apache.kafka.common.serialization.BytesDeserializer"
    "org.apache.kafka.common.serialization.DoubleDeserializer"
    "org.apache.kafka.common.serialization.FloatDeserializer"
    "org.apache.kafka.common.serialization.IntegerDeserializer"
    "org.apache.kafka.common.serialization.ListDeserializer"
    "org.apache.kafka.common.serialization.LongDeserializer"
    "org.apache.kafka.common.serialization.ShortDeserializer"
    "org.apache.kafka.common.serialization.StringDeserializer"
    "org.apache.kafka.common.serialization.UUIDDeserializer"
    "org.apache.kafka.common.serialization.VoidDeserializer"
  )
  compadd "${configs[@]}"
}


function _properties() {
  local configs=(
    "print.timestamp"
    "print.key"
    "print.offset"
    "print.partition"
    "print.headers"
    "print.value"
    "key.separator"
    "line.separator"
    "headers.separator"
    "key.deserializer"
    "value.deserializer"
    "header.deserializer"
  )
  compadd -S "=" "${configs[@]}"
}

function _consumer_configs() {
  local configs=(
    "auto.commit.interval.ms"
    "auto.offset.reset"
    "bootstrap.servers"
    "check.crcs"
    "client.dns.lookup"
    "client.id"
    "connections.max.idle.ms"
    "default.api.timeout.ms"
    "enable.auto.commit"
    "exclude.internal.topics"
    "fetch.max.bytes"
    "fetch.max.wait.ms"
    "fetch.min.bytes"
    "group.id"
    "heartbeat.interval.ms"
    "interceptor.classes"
    "isolation.level"
    "key.deserializer"
    "max.partition.fetch.bytes"
    "max.poll.interval.ms"
    "max.poll.records"
    "metadata.max.age.ms"
    "metric.reporters"
    "metrics.num.samples"
    "metrics.recording.level"
    "metrics.sample.window.ms"
    "partition.assignment.strategy"
    "receive.buffer.bytes"
    "reconnect.backoff.max.ms"
    "reconnect.backoff.ms"
    "request.timeout.ms"
    "retry.backoff.ms"
    "sasl.jaas.config"
    "sasl.kerberos.kinit.cmd"
    "sasl.kerberos.min.time.before.relogin"
    "sasl.kerberos.service.name"
    "sasl.kerberos.ticket.renew.jitter"
    "sasl.kerberos.ticket.renew.window.factor"
    "sasl.mechanism"
    "security.protocol"
    "send.buffer.bytes"
    "session.timeout.ms"
    "ssl.cipher.suites"
    "ssl.enabled.protocols"
    "ssl.endpoint.identification.algorithm"
    "ssl.key.password"
    "ssl.keymanager.algorithm"
    "ssl.keystore.location"
    "ssl.keystore.password"
    "ssl.keystore.type"
    "ssl.protocol"
    "ssl.provider"
    "ssl.secure.random.implementation"
    "ssl.truststore.location"
    "ssl.truststore.password"
    "ssl.truststore.type"
    "value.deserializer"
  )
  compadd -S "=" "${configs[@]}"
}


_script() {

  local context state

_arguments -S -s : \
  '--bootstrap-server[ the Kafka server to connect to.]:server:->bootstrap-server' \
  '*--consumer-property[ user-defined properties in the form of key=value to the consumer.]:config:_consumer_configs' \
  '--consumer.config[ consumer config property file.]:file:_files' \
  '--enable-system-events[ log lifecycle events of the consumer in addition to logging consumed messages. (This is specific for system tests.).]' \
  '--formatter[ the name of a class to use for formatting kafka messages for display. (default: kafka.tools.DefaultMessageFormatter).]' \
  '--formatter-config[ config properties file to initialize the message formatter. note that `property` takes precedence over this config.]:file:_files' \
  '--from-beginning[ if the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message.]' \
  '--group[ the consumer group id of the consumer.]:group:_groups' \
  '--include[ Regular expression specifying list of topics to include for consumption.]:regex' \
  '--isolation-level[ set to read_committed in order to filter out transactional messages which are not committed. Set to read_uncommitted to read all messages. (default: read_uncommitted).]:string' \
  '--key-deserializer[ key deserializer.]:deserializer:_deserializer' \
  '--max-messages[ the maximum number of messages to consume before exiting. If not set, consumption is continual.]:string' \
  '--offset[ the offset to consume from (a non-negative number), or 'earliest' which means from beginning, or 'latest' which means from end (default: latest).]:string' \
  '--partition[ the partition to consume from. Consumption starts from the end of the partition unless '--offset' is specified.]:string' \
  '*--property[ the properties to initialize the message formatter.]:property:_properties' \
  '--timeout-ms[ if specified, exit if no message is available for consumption for the specified interval.]:timeout' \
  '--skip-message-on-error[ if there is an error when processing a message, skip it instead of halt.]' \
  '--topic[ the topic to consume on.]:topic:_topics' \
  '--value-deserializer[ value deserializer.]:deserializer:_deserializer' \
  '--version[ display Kafka version.]' && ret=0

  case $state in
    (bootstrap-server)
      compadd "localhost:9092"
      ;;
  esac

  return $ret
}

_script "$@"

